{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import numpy as np\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gym[box2d]\n",
    "import gym\n",
    "import Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "TITAN Xp\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:{}'.format(GPU_NUM) if use_cuda else \"cpu\")\n",
    "transition = np.dtype([('s', np.float64, (4, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
    "                       ('r', np.float64), ('s_', np.float64, (4, 96, 96))])\n",
    "gamma = 0.99\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy,self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "                                    nn.Conv2d(4,8,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.MaxPool2d(kernel_size=2), #48*48*8\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(8,16,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.MaxPool2d(kernel_size=2), #24*24*16\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(16,32,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.MaxPool2d(kernel_size=2), #12*12*32\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.MaxPool2d(kernel_size=2), #6*6*64\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.MaxPool2d(kernel_size=2), #3*3*128\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(128,256,kernel_size=3,padding=0), #1*1*256\n",
    "                                    nn.ReLU()\n",
    "                                )\n",
    "        self.fc_v = nn.Sequential(nn.Linear(256,100), nn.ReLU(),nn.Linear(100,1)) # critic -> for getting value function \n",
    "        self.fc_1 = nn.Sequential(nn.Linear(256,100),nn.ReLU()) # Actor -> For getting action\n",
    "        self.fc_alpha = nn.Sequential(nn.Linear(100,3),nn.Softplus())\n",
    "        self.fc_beta = nn.Sequential(nn.Linear(100,3),nn.Softplus())\n",
    "        self.apply(self._init_weight)\n",
    "       \n",
    "        \n",
    "    @staticmethod\n",
    "    def _init_weight(m): # initializing weight\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(-1,256)\n",
    "        v = self.fc_v(x)\n",
    "        x = self.fc_1(x)\n",
    "        alpha = self.fc_alpha(x) + 1 # Beta distribution parameter must be greater than 1\n",
    "        beta = self.fc_beta(x) + 1   # Beta distribution parameter must be greater than 1\n",
    "        \n",
    "        return (alpha, beta), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    buffer_capacity = 2000 # Save 2000 sequence because max is almost 800 in CarRacing-v0\n",
    "    ppo_epoch = 10\n",
    "    batch_size = 64\n",
    "    clip_param = 0.1 # epsilon in PPO\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.net = Policy().double().to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr = 1e-3)\n",
    "        self.training_step = 0\n",
    "        self.buffer = np.empty(self.buffer_capacity, dtype=transition)\n",
    "        self.counter = 0\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            alpha, beta = self.net(state)[0]\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample() # 3 values in [0,1]\n",
    "        a_logp = dist.log_prob(action).sum(dim=1) # For PPO\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        \n",
    "        return action, a_logp\n",
    "      \n",
    "    def save_param(self):\n",
    "        torch.save(self.net.state_dict(), '/home/jaekyungcho/DRL/CarRacing/model_param/CarRacing_PPO.pkl')\n",
    "      \n",
    "    def store(self, transition):\n",
    "        self.buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity: # If buffer is full return True\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            return False  # Buffer is not full\n",
    "        \n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "        \n",
    "        s = torch.tensor(self.buffer['s'], dtype = torch.double).to(device)\n",
    "        a = torch.tensor(self.buffer['a'], dtype = torch.double).to(device)\n",
    "        r = torch.tensor(self.buffer['r'], dtype = torch.double).to(device).view(-1,1)\n",
    "        s_ = torch.tensor(self.buffer['s_'], dtype = torch.double).to(device)\n",
    "        \n",
    "        old_a_logp = torch.tensor(self.buffer['a_logp'], dtype = torch.double).to(device).view(-1,1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Using TD(1)\n",
    "            target_v = r + gamma * self.net(s_)[1] # target value function = reward + value of previous state\n",
    "            adv = target_v - self.net(s)[1]  # Advantage estimator\n",
    "            # adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(self.ppo_epoch): # Iterate\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "                # index는 buffer_capacity를 랜덤으로 섞은 뒤 batch_size로 묶은 것\n",
    "\n",
    "                (alpha, beta), value = self.net(s[index])\n",
    "                dist = Beta(alpha, beta)\n",
    "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True) # 현재 state로 구한 action의 probability\n",
    "                ratio = torch.exp(a_logp - old_a_logp[index])   # surrogate function pi(a_t|s_t)/pi_old(a_t|s_t)\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index] # clipping (0.9~1.1 사이의 ratio로만 policy update를 진행하겠다)\n",
    "                action_loss = -torch.min(surr1, surr2).mean() # clipped와 unclipped 중 작은 값을 선택 그렇게 함으로써, 너무 큰 변화 없도록 \n",
    "                value_loss = F.smooth_l1_loss(value, target_v[index]) # smooth l1 loss : [-1,1]범위는 l2 loss 사용\n",
    "                loss = action_loss + 2. * value_loss # value_loss의 제곱이 되어야 하지만 그냥 l1 norm을 사용한듯 하다.\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # nn.utils.clip_grad_norm_(self.net.parameters(), self.max_grad_norm) # RNN 같은 계열에서 사용해주는 방법 (Gradient가 너무 많이 움직이지 않도록)\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CarRacing-v0')\n",
    "        self.env.seed(0)\n",
    "        self.reward_threshold = self.env.spec.reward_threshold # Car Racing 의 경우 900\n",
    "    \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "        \n",
    "        self.die = False\n",
    "        img_rgb = self.env.reset()\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * 4\n",
    "        \n",
    "        return np.array(self.stack)\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(8): # action 을 반복해서 하는 이유는 무엇인가??\n",
    "            img_rgb, reward, die, _ = self.env.step(action)\n",
    "            # don't penalize \"die state\"\n",
    "            if die:\n",
    "                reward += 100\n",
    "            # green penalty\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "            total_reward += reward # CarRacing reward = -0.1/frame\n",
    "            # if no reward recently, end the episode, 100턴동안 안끝나면\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "            if done or die:\n",
    "                break\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == 4\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "        \n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        # rgb image -> gray [0, 1]\n",
    "        gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "        if norm:\n",
    "            # normalize\n",
    "            gray = gray / 128. - 1.\n",
    "        return gray\n",
    "    \n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        # record reward for last 100 steps\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "def show_score(x,y,i_ep):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(4)\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    plt.title(\"CarRacing | episode: {}\".format(i_ep))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1077..1357 -> 280-tiles track\n",
      "Track generation: 1322..1664 -> 342-tiles track\n",
      "Track generation: 1035..1297 -> 262-tiles track\n",
      "Track generation: 1297..1626 -> 329-tiles track\n",
      "Track generation: 1294..1621 -> 327-tiles track\n",
      "Track generation: 1055..1331 -> 276-tiles track\n",
      "Track generation: 1202..1507 -> 305-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c87710436d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 원래대로라면 t=738에서 끝나야하므로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m#if render_env:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2e49e6d281a0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# action 을 반복해서 하는 이유는 무엇인가??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mimg_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# don't penalize \"die state\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdie\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglViewport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender_road\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         vl = pyglet.graphics.vertex_list(\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygons_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"v3f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygons_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"c4f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gl.GL_QUADS,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         )\n\u001b[1;32m    530\u001b[0m         \u001b[0mvl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_QUADS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/pyglet/graphics/__init__.py\u001b[0m in \u001b[0;36mvertex_list\u001b[0;34m(count, *data)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# Note that mode=0 because the default batch is never drawn: vertex lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# returned from this function are drawn directly by the app.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_default_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/pyglet/graphics/__init__.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, count, mode, group, *data)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mvlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitial_arrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mvlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_attribute_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaekyungcho/anaconda3/envs/pytorch_3.5/lib/python3.5/site-packages/pyglet/graphics/vertexdomain.py\u001b[0m in \u001b[0;36m_set_attribute_data\u001b[0;34m(self, i, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# TODO without region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_env = True\n",
    "score_x = []\n",
    "av_score = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    env = Env()\n",
    "\n",
    "    training_records = []\n",
    "    running_score = 0\n",
    "    for i_ep in range(100000): # episode iteration\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1000): # 원래대로라면 t=738에서 끝나야하므로\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "            #if render_env:\n",
    "                #env.render()\n",
    "                #show_state(env.env)\n",
    "            if agent.store((state, action, a_logp, reward, state_)): # buffer is full (2000) -> minimum 2 episode\n",
    "                print('updating')\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "            if done or die:\n",
    "                break\n",
    "        running_score = running_score * 0.99 + score * 0.01 # moving average\n",
    "\n",
    "        if i_ep % 10 == 0:\n",
    "#             if args.vis:\n",
    "#                 draw_reward(xdata=i_ep, ydata=running_score)\n",
    "            agent.save_param()\n",
    "            score_x.append(i_ep)\n",
    "            av_score.append(running_score)\n",
    "            show_score(score_x,av_score,i_ep)\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "            \n",
    "        if running_score > env.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {}!\".format(running_score, score))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
